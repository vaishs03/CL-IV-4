{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1fb96-360e-4c1c-9381-acfb29f8c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3. Design RNN or its variant including LSTM or GRU a) Select a suitable time series dataset.\n",
    "Example – predict sentiments based on product reviews b) Apply for prediction'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616bacd2-6422-46b7-b72b-a4381d5d6814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\New\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940eaa77-d6bd-40c1-a295-f0645f052d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n            Sequential: A linear stack of layers to build the model.\\n            Embedding: Converts integer-encoded words into dense vectors (e.g., \"cat\" → [0.2, -0.5, ...]).\\n            LSTM: Layer to process sequential data with memory cells and gates.\\n            Dense: Fully connected layer for classification.\\n            imdb: Preloaded dataset of movie reviews labeled as positive (1) or negative (0).\\n            sequence: Utilities for padding sequences to a fixed length. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explanation:\n",
    "\"\"\" \n",
    "            Sequential: A linear stack of layers to build the model.\n",
    "            Embedding: Converts integer-encoded words into dense vectors (e.g., \"cat\" → [0.2, -0.5, ...]).\n",
    "            LSTM: Layer to process sequential data with memory cells and gates.\n",
    "            Dense: Fully connected layer for classification.\n",
    "            imdb: Preloaded dataset of movie reviews labeled as positive (1) or negative (0).\n",
    "            sequence: Utilities for padding sequences to a fixed length. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a5313b-d535-4fca-b982-fc81cc254868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "vocab_size = 5000  # Use top 5,000 frequent words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc17356-702c-4af7-b9c5-aa50f5fc064e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\\n            imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\\n            x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\\n            y_train/y_test: Labels (0 or 1).\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Explanation:\n",
    "\"\"\"\n",
    "            vocab_size=5000: Restrict vocabulary to the 5,000 most frequent words (reduces noise from rare words).\n",
    "            imdb.load_data(): Loads the IMDB dataset preprocessed into integer sequences.\n",
    "            x_train/x_test: Lists of reviews, where each word is replaced by its integer index.\n",
    "            y_train/y_test: Labels (0 or 1).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3b266b-cf7a-4bdf-b325-e7abe083816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to fixed length (400 words)\n",
    "max_words = 400\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b44ba02-db94-4b01-a7ca-aea2c1a23718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            Explanation:\\n            max_words=400: Truncate/pad all reviews to 400 words.\\n            Shorter reviews are padded with zeros (e.g., [0, 0, ..., 12, 42]).\\n            Longer reviews are truncated to 400 words.\\n            Why? Neural networks require fixed-length inputs for batch processing.\\n    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "            Explanation:\n",
    "            max_words=400: Truncate/pad all reviews to 400 words.\n",
    "            Shorter reviews are padded with zeros (e.g., [0, 0, ..., 12, 42]).\n",
    "            Longer reviews are truncated to 400 words.\n",
    "            Why? Neural networks require fixed-length inputs for batch processing.\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41fe5d7-b2b9-458d-82b7-95aacbbf0428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\New\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential(name=\"LSTM_Sentiment_Analysis\")\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_words))  # Convert word indices to 32D vectors\n",
    "model.add(LSTM(128, activation='tanh', return_sequences=False))  # 128 LSTM units\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e8ed21b-a9c9-4038-90fc-aef41c390bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            Step 1: Embedding Layer:\\n            vocab_size: 5,000 unique words.\\n            32: Embedding dimension (each word is a 32-dimensional vector).\\n            input_length=max_words: Each input sequence has 400 words.\\n            Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\\n\\n            Step 2: LSTM Layer:\\n            128: Number of LSTM units (dimensionality of the hidden state).\\n            activation=\\'tanh\\': Hyperbolic tangent activation for gate updates.\\n            return_sequences=False: Return only the final output (not all timesteps).\\n            Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\\n\\n            Step 3: Dense Layer:\\n            1: Single neuron for binary classification (positive/negative).\\n            activation=\\'sigmoid\\': Squashes output to [0, 1] (probability of positive sentiment).\\n\\n    '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "            Step 1: Embedding Layer:\n",
    "            vocab_size: 5,000 unique words.\n",
    "            32: Embedding dimension (each word is a 32-dimensional vector).\n",
    "            input_length=max_words: Each input sequence has 400 words.\n",
    "            Purpose: Converts sparse integer-encoded words into dense vectors that capture semantic meaning (e.g., \"good\" and \"great\" are closer in vector space).\n",
    "\n",
    "            Step 2: LSTM Layer:\n",
    "            128: Number of LSTM units (dimensionality of the hidden state).\n",
    "            activation='tanh': Hyperbolic tangent activation for gate updates.\n",
    "            return_sequences=False: Return only the final output (not all timesteps).\n",
    "            Purpose: Processes the sequence word-by-word, updating its hidden state to capture context.\n",
    "\n",
    "            Step 3: Dense Layer:\n",
    "            1: Single neuron for binary classification (positive/negative).\n",
    "            activation='sigmoid': Squashes output to [0, 1] (probability of positive sentiment).\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d17d45c7-8183-44c6-a3f7-2c891901b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\New\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11d8cc38-a0f5-4de7-b768-e906d3291be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        Explanation:\\n        loss='binary_crossentropy': Standard loss for binary classification.\\n        optimizer='adam': Adaptive learning rate optimizer (efficient for RNNs).\\n        metrics=['accuracy']: Track accuracy during training.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "            Explanation:\n",
    "            loss='binary_crossentropy': Standard loss for binary classification.\n",
    "            optimizer='adam': Adaptive learning rate optimizer (efficient for RNNs).\n",
    "            metrics=['accuracy']: Track accuracy during training.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9830c64-cad6-429e-ae61-19067e7c20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\New\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\New\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "313/313 [==============================] - 190s 596ms/step - loss: 0.5227 - accuracy: 0.7275 - val_loss: 0.4260 - val_accuracy: 0.8070\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 184s 587ms/step - loss: 0.3311 - accuracy: 0.8648 - val_loss: 0.3285 - val_accuracy: 0.8650\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 187s 598ms/step - loss: 0.2873 - accuracy: 0.8834 - val_loss: 0.3286 - val_accuracy: 0.8622\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 194s 619ms/step - loss: 0.2341 - accuracy: 0.9074 - val_loss: 0.3298 - val_accuracy: 0.8704\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 195s 622ms/step - loss: 0.1850 - accuracy: 0.9315 - val_loss: 0.3338 - val_accuracy: 0.8648\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(x_train, y_train, \n",
    "                   batch_size=64, \n",
    "                   epochs=5, \n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d00118-2012-48e9-a4af-21abd58a06ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        Explanation:\\n        batch_size=64: Update weights after every 64 samples (balance speed/memory).\\n        epochs=5: Train for 5 full passes over the training data.\\n        validation_split=0.2: Use 20% of training data for validation (monitor overfitting).\\n        Output: Training logs show loss/accuracy for training and validation sets.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "            Explanation:\n",
    "            batch_size=64: Update weights after every 64 samples (balance speed/memory).\n",
    "            epochs=5: Train for 5 full passes over the training data.\n",
    "            validation_split=0.2: Use 20% of training data for validation (monitor overfitting).\n",
    "            Output: Training logs show loss/accuracy for training and validation sets.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fc3d18-a875-48f6-8a8d-4e9435eabcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.74%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea0d92d-2711-4292-9678-2567a4ba2602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        Explanation:\\n        evaluate(): Computes loss and accuracy on unseen test data.\\n        test_acc: Accuracy reflects how well the model generalizes to new reviews.\\n        Typical Output: ~80-88% accuracy depending on hyperparameters.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "            Explanation:\n",
    "            evaluate(): Computes loss and accuracy on unseen test data.\n",
    "            test_acc: Accuracy reflects how well the model generalizes to new reviews.\n",
    "            Typical Output: ~80-88% accuracy depending on hyperparameters.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9721921-ad10-43f7-ac53-5d5dff0ed72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
